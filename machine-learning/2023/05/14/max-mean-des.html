<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Maximum Mean Discrepancy and Reproduce Kernel Hilbert Space | Jianting Feng</title> <meta name="author" content="Jianting Feng"> <meta name="description" content="Introduction of Maximum Mean Discrepancy"> <meta name="keywords" content="Statistical Machine Learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%9D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jiantingfeng.github.io/machine-learning/2023/05/14/max-mean-des.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jianting </span>Feng</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/code/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Maximum Mean Discrepancy and Reproduce Kernel Hilbert Space</h1> <p class="post-meta">May 14, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fas fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <h2 id="intuition">Intuition</h2> <p>The concept of <em>Maximum Mean Discrepancy</em> is introduced in <a href="https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf" rel="external nofollow noopener" target="_blank">gretton12a</a> to solve following question:</p> <p>Let \(X\) and \(Y\) be random variables defined on a topological space \(\mathcal{X}\) , with respective Borel probability measure \(p\) and \(q\) . Given observations \(X = \{X_1,\cdots, X_n\}\) and \(Y=\{Y_1, \cdots, Y_m\}\) i.i.d. from \(p\) and \(q\) , respectively. <strong>How can we decided whether</strong> \(p\neq q\)?</p> <p>In other words, we need to measure the <strong>distance</strong> between two group of samples.</p> <h2 id="technique-details">Technique Details</h2> <p>First we have following lemma,</p> <blockquote> <p>Let \((\mathcal X, d)\) be a metric space, and \(p, q\) be two Borel probability measures defined on \(\mathcal X\) . Then \(p=q\) if and only if \(\mathbb{E}_X[f(X)]=\mathbb{E}_Y[f(Y)]\) for all \(f\in C(\mathcal X)\) . Where \(C(\mathcal X)\) denoted the space of bounded continuous functions on \(\mathcal X\).</p> </blockquote> <p>With the help of this lemma, we can turn our problem into an equivalent form:</p> <p>Let \(\mathcal F\in C(\mathcal X)\), we define <em>maximal mean discrepancy</em> (MMD) as</p> \[\textrm{MMD}[\mathcal F, p, q] = \sup_{f\in\mathcal F}\left(\mathbb{E}_X[f(X)] - \mathbb{E}_Y(f(Y)\right)\] <p>where \(x\sim p\) and \(y\sim q\), then \(p = q\) <strong>if and only</strong> \(\textrm{MMD}[\mathcal F, p, q] = 0\).</p> <p><em>MMD</em> is also referred as <strong>integral probability metric</strong>. In practicle, we tends to use a biased empirical estimation of <em>MMD</em>:</p> \[\widehat{\textrm{MMD}}[\mathcal F, X, Y] = \sup_{\varphi\in\mathcal H}\left(\frac{1}{m}\sum_{i=1}^m\varphi(X_i) - \frac{1}{n}\sum_{i=1}^n\varphi(Y_i)\right)\] <p>Here, we restrict the function class in a unit ball of <strong>reproducing kernel Hilbert space</strong> \(\mathcal H\).</p> <p>This empiricial formula seems intractable as well because of the \(\sup\), however, we will illustrate how to get rid of that in the following context. Now, let’s first under an important concept: <em>Reproducing Kernel Hilbert Space</em>.</p> <h3 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</h3> <p>Indeed, RKHS is a ubiquitus concept in the field of Machine Learning. In order to figure out what it is, firstly we need some math foundations, I put some useful links here:</p> <ul> <li><a href="https://mathworld.wolfram.com/HilbertSpace.html" rel="external nofollow noopener" target="_blank">Hilbert Space</a></li> <li><a href="https://www.wikiwand.com/en/Riesz_representation_theorem" rel="external nofollow noopener" target="_blank">Riesz Representation Theorem</a></li> <li><a href="https://www.wikiwand.com/en/Kernel_method" rel="external nofollow noopener" target="_blank">Kernel Method</a></li> </ul> <p>The kernel function is a generalization of normal inner product in Euclidan space, with <strong>non-negative, symmetry, and bilinearily</strong>.</p> <p>Given any two data points \(x, x^\prime \in \mathcal X\), together with feature map \(\Phi: \mathcal X\to\mathcal F\subset \mathbb R^{N}\), we can define a <strong>kernel</strong> \(K: \mathcal X\times \mathcal X\to \mathbb R\) as:</p> \[\forall x, x^\prime\in\mathcal X,\qquad K(x, x^\prime) = \langle \Phi(x), \Phi(x^\prime)\rangle = \Phi(x^\prime)^T\Phi(x)\] <p>The reason why we use kernel function rather than original inner product is that: in many machine learning problems, inner product in feature space is extensively (e.g. support vector machine). Normally we don’t use the original form \(\Phi(x)\), which always appears in the form of inner product.</p> <p>then, the Reproducing Kernel Hilbert space is a Hilbert space with <strong>reproducing property</strong>:</p> \[\forall h\in\ \mathcal H, \forall x\in\mathcal X,\qquad h(x) = \langle h, K(x, \cdot)\rangle.\] <p>By the definition of kernel, partial evaluation \(K(x, \cdot)\) is a function from \(\mathcal X\) to \(\mathcal F\) therefore, from the perspective of function space, it can be viewed as the Riesz representation theorem.</p> <p>Recall the evaluation function in RKHS is bounded. By Riesz representation, for all \(x\in\mathcal H\) , \(\exists \phi(x)\in\mathcal H\) , such that the evaluation \(f(x) = \langle f, \phi(x)\rangle_{\mathcal H}\) . The feature map takes the form \(\phi(x) = k(x, \cdot)\) where \(k(\cdot, \cdot)\) is a kernel function defined as \(k(x, z) = \langle\phi(x), \phi(z)\rangle_{\mathcal{H}}\)</p> <ul> <li>This notation can also be extended into expectation, i.e. \(\exists\mu\in\mathcal H\) , \(\forall f\in \mathcal H\) , we have \(\mathbb{E}f = \langle f, \mu\rangle_{\mathcal H}\)</li> <li>We have - \(\mathrm{MMD}^2[\mathcal F, p, q] = \lVert \mu_p - \mu_q \rVert^2\) - Proof is directly followed by the Riesz repr. of expectation.</li> <li>With inner product, it can also be written as \(\langle\mu_p, \mu_p\rangle_{\mathcal H}^2 - 2\langle\mu_p, \mu_q\rangle_{\mathcal H}+\langle\mu_q, \mu_q\rangle_{\mathcal H}^2\)</li> <li>Note that \(\langle\mu_p, \mu_q\rangle_{\mathcal H} = \int_{\mathcal X\times \mathcal X}k(x, z) \mathrm d\mathbb P\mathrm d\mathbb Q\) We can expand the previous form</li> <li>With inner product, it can also be written as \(\langle\mu_p, \mu_p\rangle_{\mathcal H}^2 - 2\langle\mu_p, \mu_q\rangle_{\mathcal H}+\langle\mu_q, \mu_q\rangle_{\mathcal H}^2\) <ul> <li>as expectation of kernel function</li> <li>When do actual calculating, directly use sample expectation to replace the integral.</li> <li>Empirical \(\textrm{MMD}^2 = \frac{1}{m(m-1)}\sum_i\sum_{j\neq i}k(x_i, x_j) - \frac{2}{mn}\sum_i\sum_j k(x_i, z_j) + \frac{1}{n(n-1)}\sum_i\sum_{j\neq i}k(z_i, z_j)\)</li> </ul> </li> <li>you can use different kernel function with different hyperparameters (e,g, bandwidth for Gaussian kernel)</li> </ul> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"jiantingfeng/jiantingfeng.github.io","data-repo-id":"R_kgDOI5no5w","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jianting Feng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. All contents are under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external nofollow noopener">CC BY-NC-ND 4.0</a>. Last updated: March 06, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-DLKPXN9214"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DLKPXN9214");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>